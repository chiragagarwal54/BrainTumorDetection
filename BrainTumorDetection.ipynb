{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyJvn1Z8NyO2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMd_Vus4Npvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d182f3-4c42-4d8e-edb0-eddec2b87e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1C5uwn0N6b4"
      },
      "outputs": [],
      "source": [
        "Dataset = '/content/drive/MyDrive/BRAIN MRI DATASET'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kKOmpojN9v2"
      },
      "outputs": [],
      "source": [
        "#Importing libraries\n",
        "%matplotlib inline\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, random, shutil\n",
        "import tensorflow as tf\n",
        "import seaborn\n",
        "from glob import glob\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import preprocessing, layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras_preprocessing import image\n",
        "import PIL\n",
        "import cv2\n",
        "from keras.constraints import maxnorm\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import GlobalAveragePooling2D, Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from cv2 import cv2\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, ZeroPadding2D\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from numpy import array \n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5m0C6qMOLF1"
      },
      "outputs": [],
      "source": [
        "def getTotalImages():\n",
        "\n",
        "  total_images = 0\n",
        "  for c in ['G','M','N','P']:\n",
        "    total_images += len(os.listdir(os.path.join(Dataset, c)))\n",
        "  # print('Total:', total_images)\n",
        "  return total_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3av_vR92PK69"
      },
      "outputs": [],
      "source": [
        "def defineTumourLabels():\n",
        "  data_dir = \"\"\n",
        "  data_dir = pathlib.Path(Dataset)\n",
        "  Tumor_images_dict= {}\n",
        "  Tumor_labels_dict = {}\n",
        "  Tumor_images_dict= {\n",
        "      'G': list(data_dir.glob('G/*')),\n",
        "      'M': list(data_dir.glob('M/*')),\n",
        "      'N': list(data_dir.glob('N/*')),  \n",
        "      'P': list(data_dir.glob('P/*'))\n",
        "  }\n",
        "  Tumor_labels_dict = {\n",
        "      'G':0,\n",
        "      'M': 1,\n",
        "      'N': 2,\n",
        "      'P': 3,\n",
        "  }\n",
        "  return Tumor_images_dict,Tumor_labels_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYuadpolPLSp"
      },
      "outputs": [],
      "source": [
        "def preProcess(Tumor_images_dict,Tumor_labels_dict):\n",
        "  print(\"\\n Enter the Dataset Size\")\n",
        "  Dataset_Size = input()\n",
        "  Dataset_Size = int(Dataset_Size)\n",
        "  X, y = [],[]\n",
        "\n",
        "  for Tumor_name, images in Tumor_images_dict.items():\n",
        "      for image in images[0:Dataset_Size]:\n",
        "          img= cv2.imread(str(image))\n",
        "          im_color = cv2.applyColorMap(img, cv2.COLORMAP_JET)\n",
        "          resized_img = cv2.resize(im_color,(180,180))\n",
        "          X.append(resized_img)\n",
        "          y.append(Tumor_labels_dict[Tumor_name])\n",
        "  return X,y     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3hBX772Q4DM"
      },
      "outputs": [],
      "source": [
        "def split(X,y):\n",
        "  X =  np.array(X)\n",
        "  y = np.array(y)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0) #input 2\n",
        "  X_train_scaled = X_train / 255\n",
        "  X_test_scaled = X_test / 255\n",
        "  return X_train_scaled,X_test_scaled,y_train,y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uFlIJ0qR_XX"
      },
      "outputs": [],
      "source": [
        "def struct_model():\n",
        "    num_classes = 4;\n",
        "    model = Sequential([\n",
        "    #input 3             \n",
        "    layers.Conv2D(275,(3,3), padding='same', activation='relu', input_shape=(180,180,3)),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Conv2D(250,(3,3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Conv2D(225,(3,3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Dropout(0.2),\n",
        "    \n",
        "    layers.Flatten(),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Dense(50, activation='relu'),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Dense(num_classes,activation='softmax')  \n",
        "])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Validation_Process(model,X_test_scaled, y_test):\n",
        "  show_validation_accuracy(model,X_test_scaled, y_test)\n",
        "  cm = show_classification_report(model,X_test_scaled, y_test)\n",
        "  show_confusion_matrix(cm)"
      ],
      "metadata": {
        "id": "tb9-xJohcn5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By17bAufUv0G"
      },
      "outputs": [],
      "source": [
        "def train(model,X_train_scaled, y_train,X_test_scaled, y_test):\n",
        "  model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
        "  #input 4\n",
        "  early_stop = EarlyStopping(monitor = 'val_accuracy', mode = 'max', patience=10, restore_best_weights=True)\n",
        "  history = model.fit(X_train_scaled, y_train, epochs=20, validation_data= (X_test_scaled, y_test))\n",
        "  return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izDnjRULwfY1"
      },
      "outputs": [],
      "source": [
        "def print_model_summary(model):\n",
        "  model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tfg5vTpVE-p"
      },
      "outputs": [],
      "source": [
        "def show_validation_accuracy(model,X_test_scaled, y_test):\n",
        "  model.evaluate(X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08cLF29DwhNT"
      },
      "outputs": [],
      "source": [
        "def show_classification_report(model,X_test_scaled, y_test):\n",
        "  from sklearn.metrics import confusion_matrix , classification_report\n",
        "  import numpy as np\n",
        "  y_predict =  model.predict(X_test_scaled)\n",
        "  y_predict_classes = [np.argmax(element) for element in y_predict]\n",
        "  print(\"Classification Report: \\n\", classification_report(y_test, y_predict_classes))\n",
        "  cm = tf.math.confusion_matrix(labels=y_test,predictions=y_predict_classes)\n",
        "  return cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdeGrrGDwppc"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(cm):\n",
        "  import seaborn as sn\n",
        "  plt.figure(figsize = (10,8))\n",
        "  sn.heatmap(cm, annot=True, fmt='d')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('Truth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Learning_Process():\n",
        "  print(\"Learning...\")\n",
        "  total_images = getTotalImages()\n",
        "  print(total_images)\n",
        "  Tumor_images_dict,Tumor_labels_dict = defineTumourLabels()\n",
        "  print(\"Preprocessing...\")\n",
        "  X,y = preProcess(Tumor_images_dict,Tumor_labels_dict)\n",
        "  print(\"Preprocessing Done!\")\n",
        "  X_train_scaled,X_test_scaled, y_train, y_test = split(X,y)\n",
        "  model = struct_model()\n",
        "  print_model_summary(model)\n",
        "  history = train(model,X_train_scaled, y_train,X_test_scaled, y_test)\n",
        "  print(\"Learned!\")\n",
        "  print(\"Validation on process...\")\n",
        "  Validation_Process(model, X_test_scaled, y_test)\n"
      ],
      "metadata": {
        "id": "3o2sbR9EnHQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def names(number):\n",
        "    if number==0:\n",
        "        return 'Diagnosed with Glioma Tumor'\n",
        "    elif number==1:\n",
        "        return 'Diagnosed with Meningioma Tumor'\n",
        "    elif number==2:\n",
        "        return 'Diagnosed with No Tumor'\n",
        "    else:\n",
        "        return 'Diagnosed with Pituitary Tumor'"
      ],
      "metadata": {
        "id": "puW5pF0QcIh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Recognition_Process():\n",
        "  from matplotlib.pyplot import imshow\n",
        "  print(\"Recongnition...\")\n",
        "  print(\"Enter image path for recognition:\")\n",
        "  imgpath = input()\n",
        "  img = Image.open(imgpath)\n",
        "  x = np.array(img.resize((180,180)))\n",
        "  x = x.reshape(1,180,180,3)\n",
        "  res = model.predict_on_batch(x)\n",
        "  classification = np.where(res == np.amax(res))[1][0]\n",
        "  imshow(img)\n",
        "  print(str(res[0][classification]*100) + '% Confidence This Is ' + names(classification))"
      ],
      "metadata": {
        "id": "B9fqNq_gZrSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  print(\"Welcome to User Interaction Menu\")\n",
        "  print(\"--------------------------------\")\n",
        "  var = 2\n",
        "  while(var==1 or var==2):\n",
        "    print(\"1.Learning Phase\")\n",
        "    print(\"2.Recognition Phase\")\n",
        "    print(\"3.End Program\")\n",
        "    print(\"Enter an option:\")\n",
        "    var = input()\n",
        "    var = int(var)\n",
        "    if var == 1:\n",
        "      print(\"Starting learning...\")\n",
        "      Learning_Process()\n",
        "    elif var == 2:\n",
        "      Recognition_Process()"
      ],
      "metadata": {
        "id": "FX5ycDL4lOVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "PZdKYAs4mwBJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a630e9d0-776d-47c0-c4c8-3f7414e2104e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to User Interaction Menu\n",
            "--------------------------------\n",
            "1.Learning Phase\n",
            "2.Recognition Phase\n",
            "3.End Program\n",
            "Enter an option:\n",
            "1\n",
            "Starting learning...\n",
            "Learning...\n",
            "3319\n",
            "Preprocessing...\n",
            "\n",
            " Enter the Dataset Size\n",
            "50\n",
            "Preprocessing Done!\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 180, 180, 275)     7700      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 90, 90, 275)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 90, 90, 275)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 90, 90, 250)       619000    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 45, 45, 250)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 45, 45, 250)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 45, 45, 225)       506475    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 22, 22, 225)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 22, 22, 225)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 108900)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               10890100  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 204       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,028,529\n",
            "Trainable params: 12,028,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 16s 493ms/step - loss: 2.1210 - accuracy: 0.1375 - val_loss: 1.3883 - val_accuracy: 0.2750\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 1s 306ms/step - loss: 1.3836 - accuracy: 0.2688 - val_loss: 1.3391 - val_accuracy: 0.4000\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 1s 304ms/step - loss: 1.2399 - accuracy: 0.4187 - val_loss: 1.0020 - val_accuracy: 0.7750\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 1s 305ms/step - loss: 0.9249 - accuracy: 0.5875 - val_loss: 0.6488 - val_accuracy: 0.7500\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 1s 302ms/step - loss: 0.8596 - accuracy: 0.7125 - val_loss: 0.6826 - val_accuracy: 0.7000\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 1s 304ms/step - loss: 0.7390 - accuracy: 0.7125 - val_loss: 0.6585 - val_accuracy: 0.7500\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 1s 304ms/step - loss: 0.8145 - accuracy: 0.7250 - val_loss: 0.5695 - val_accuracy: 0.8000\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 1s 303ms/step - loss: 0.6611 - accuracy: 0.7750 - val_loss: 0.5563 - val_accuracy: 0.7750\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 2s 312ms/step - loss: 0.5688 - accuracy: 0.7812 - val_loss: 0.5738 - val_accuracy: 0.8500\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 1s 303ms/step - loss: 0.4883 - accuracy: 0.8250 - val_loss: 0.5781 - val_accuracy: 0.8000\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 1s 304ms/step - loss: 0.4159 - accuracy: 0.8625 - val_loss: 0.6346 - val_accuracy: 0.8750\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 1s 305ms/step - loss: 0.3877 - accuracy: 0.8687 - val_loss: 0.5216 - val_accuracy: 0.8500\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 1s 307ms/step - loss: 0.3109 - accuracy: 0.8938 - val_loss: 0.5666 - val_accuracy: 0.8250\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 2s 307ms/step - loss: 0.3031 - accuracy: 0.9125 - val_loss: 0.6696 - val_accuracy: 0.8250\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 1s 305ms/step - loss: 0.2177 - accuracy: 0.9312 - val_loss: 0.7465 - val_accuracy: 0.8250\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 2s 306ms/step - loss: 0.3118 - accuracy: 0.9062 - val_loss: 0.8417 - val_accuracy: 0.7750\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 1s 306ms/step - loss: 0.2232 - accuracy: 0.9250 - val_loss: 0.7686 - val_accuracy: 0.7750\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 2s 308ms/step - loss: 0.2436 - accuracy: 0.9125 - val_loss: 0.7966 - val_accuracy: 0.8500\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 2s 307ms/step - loss: 0.2739 - accuracy: 0.9250 - val_loss: 0.8394 - val_accuracy: 0.8000\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 1s 306ms/step - loss: 0.1924 - accuracy: 0.9250 - val_loss: 0.7458 - val_accuracy: 0.8000\n",
            "Learned!\n",
            "Validation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-68fc34c81b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-92-4315770a798a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mLearning_Process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mValidation_Process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mRecognition_Process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Validation_Process() missing 3 required positional arguments: 'model', 'X_test_scaled', and 'y_test'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BrainTumorDetection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}